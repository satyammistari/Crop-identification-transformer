# @package trainer
_target_: pytorch_lightning.Trainer

# Training configuration
max_epochs: 100
min_epochs: 10
check_val_every_n_epoch: 1
log_every_n_steps: 50

# Hardware configuration
accelerator: "gpu"
devices: 1
strategy: "auto"
precision: "16-mixed"  # Mixed precision training

# Gradient management
gradient_clip_val: 1.0
gradient_clip_algorithm: "norm"
accumulate_grad_batches: 1

# Validation and testing
limit_train_batches: 1.0
limit_val_batches: 1.0
limit_test_batches: 1.0
val_check_interval: 1.0

# Reproducibility
deterministic: true
benchmark: false

# Performance optimization
enable_model_summary: true
enable_checkpointing: true
enable_progress_bar: true
default_root_dir: "outputs"

# Debugging (disable for production)
fast_dev_run: false
overfit_batches: 0
profiler: null  # Set to "simple" or "advanced" for profiling

# Distributed training (for multi-GPU)
sync_batchnorm: false
replace_sampler_ddp: true
